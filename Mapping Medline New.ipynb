{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/Ganesh P Umarani/Desktop/Optum/pm/Medline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file with weblinks to product\n",
    "medline=pd.read_excel('Medline_sourcing_done.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Secondary_Key', 'Link Owner's', 'DISTRIBUTOR',\n",
       "       'DISTRIBUTOR PRODUCT NUMBER', 'Manufacturer', 'mnf_sourced ',\n",
       "       'Mfg_part_num', 'mpn_sourced', 'Product Description', 'Cluster Iter4',\n",
       "       'Cluster ID', 'Weblink', 'Sourcing Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows where there are no weblinks\n",
    "medline=medline[medline.Weblink.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medline_sample=medline.iloc[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the output dataframe to store the scrapped results\n",
    "output=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare a dataframe to store urls which ran into issues so that ther can be looked upon later\n",
    "issue_url=pd.DataFrame(columns=['Weblinks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length---> 1\n",
      "Output length---> 2\n"
     ]
    }
   ],
   "source": [
    "#loop through each row of the uploaded file and get the weblink to send the request\n",
    "for idx,row in medline.iterrows():\n",
    "    url=row['Weblink'].strip()\n",
    "    try:\n",
    "        data=requests.get(url) #Send request and capture response\n",
    "    except:\n",
    "        print(\"Issue with  {}\".format(url))\n",
    "        issue_url=issue_url.append({'Weblinks':url},ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    #check if the status returened on sending request is 200 which means to check if we got any response\n",
    "    if(data.status_code==200):\n",
    "        data_pretty=BeautifulSoup(data.text,'html.parser') #Parse the response to html format\n",
    "        to_append={} # declare a dict to store the values scrapped in key value format, later this dict will be appended to output dataframe\n",
    "        \n",
    "        #Below snippet scraps long and short description and also product explaination\n",
    "        try:\n",
    "            descriptions=data_pretty.find_all('div',{'class':'medSKUDescription'})\n",
    "            ldesc=descriptions[0].select('h1')[0].select('a')[0].contents[0]\n",
    "            sdesc=descriptions[0].select('h4')[0].contents[0]\n",
    "            prod_exp=descriptions[0].find_all('div',{'class':'medMoreSKUDescText'})[0].contents[0].strip()\n",
    "            to_append['Long_desc']=ldesc\n",
    "            to_append['Short_desc']=sdesc\n",
    "            to_append['Product_details']=prod_exp\n",
    "        except:\n",
    "            print(\"Issue with  {}\".format(url))\n",
    "            issue_url=issue_url.append({'Weblinks':url},ignore_index=True)\n",
    "            continue\n",
    "        \n",
    "        #below snippet will scrape the details of the product which are given in a table\n",
    "        try:\n",
    "            detail_table=data_pretty.find_all('div',{'class':'medSKUFltRt'})[0].select_one('table',{'class':'medSKUTableDetails'})\n",
    "            list_of_tr=detail_table.find_all('tr')\n",
    "            \n",
    "            for i in range(len(list_of_tr)):\n",
    "                list_of_td=list_of_tr[i].find_all('td')\n",
    "                to_append[list_of_td[0].contents[0].strip()]=list_of_td[1].select('span')[0].contents[0].strip()\n",
    "        except:\n",
    "            print(\"Issue with  {}\".format(url))\n",
    "            issue_url=issue_url.append({'Weblinks':url},ignore_index=True)\n",
    "            continue\n",
    "        \n",
    "        #Below snippet will scrape the specification of the product\n",
    "        try:\n",
    "            spec_table=data_pretty.find_all('table',{'id':'medSkuSpecTable'})\n",
    "            list_of_tr_st=spec_table[0].find_all('tr')\n",
    "            \n",
    "            for k in range(len(list_of_tr_st)):\n",
    "                list_of_td_st=list_of_tr_st[k].find_all('td')\n",
    "                to_append[list_of_td_st[0].contents[0].strip()]=list_of_td_st[1].contents[0].strip()\n",
    "        except:\n",
    "            print(\"Issue with  {}\".format(url))\n",
    "            issue_url=issue_url.append({'Weblinks':url},ignore_index=True)\n",
    "            continue\n",
    "        \n",
    "        # All the columns from the i/p file which are needed in o/p file are included in the dict\n",
    "        to_append['Secondar_key']=row['Secondary_Key']\n",
    "        to_append['DISTRIBUTOR']=row['DISTRIBUTOR']\n",
    "        to_append['DISTRIBUTOR PRODUCT NUMBER']=row['DISTRIBUTOR PRODUCT NUMBER']\n",
    "        to_append['Manufacturer']=row['Manufacturer']\n",
    "        to_append['mnf_sourced ']=row['mnf_sourced ']\n",
    "        to_append['Mfg_part_num']=row['Mfg_part_num']\n",
    "        to_append['mpn_sourced']=row['mpn_sourced']\n",
    "        to_append['Product Description']=row['Product Description']\n",
    "        to_append['Weblink']=row['Weblink']\n",
    "        \n",
    "        #Append the dict to the o/p dataframe\n",
    "        output=output.append(to_append,ignore_index=True)\n",
    "        print(\"Output length---> {}\".format(len(output)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
